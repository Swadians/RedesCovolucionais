{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceGlassLoadDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    # Read a PGM file and return a list of lists (each list is a line)\n",
    "    def read_pgm(self, pgmf):\n",
    "        assert pgmf.readline() == b'P5\\n'\n",
    "        (width, height) = [int(i) for i in pgmf.readline().split()]\n",
    "        depth = int(pgmf.readline())\n",
    "        assert depth <= 255\n",
    "\n",
    "        raster = []\n",
    "        for y in range(height):\n",
    "            for y in range(width):\n",
    "                raster.append(np.array(ord(pgmf.read(1))))\n",
    "        return np.array(raster)\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        # Read all images to build dataset\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for filename in glob.glob(root_dir + \"*.pgm\"):\n",
    "            if \"sunglasses\" in filename:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            image = Image.open(filename)\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.imagesData = images        \n",
    "        self.labelsData = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagesData)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'image': self.imagesData[idx], 'label': self.labelsData[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            sample['label'] = torch.from_numpy(np.array(sample['label'], dtype='long')).long()           \n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDirectionLoadDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    # Read a PGM file and return a list of lists (each list is a line)\n",
    "    def read_pgm(self, pgmf):\n",
    "        assert pgmf.readline() == b'P5\\n'\n",
    "        (width, height) = [int(i) for i in pgmf.readline().split()]\n",
    "        depth = int(pgmf.readline())\n",
    "        assert depth <= 255\n",
    "\n",
    "        raster = []\n",
    "        for y in range(height):\n",
    "            for y in range(width):\n",
    "                raster.append(np.array(ord(pgmf.read(1))))\n",
    "        return np.array(raster)\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        # Read all images to build dataset\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for filename in glob.glob(root_dir + \"*.pgm\"):\n",
    "            if \"left\" in filename:\n",
    "                label = 0\n",
    "            elif \"right\" in filename:\n",
    "                label = 1\n",
    "            elif \"straight\" in filename:\n",
    "                label = 2\n",
    "            elif \"up\" in filename:\n",
    "                label = 3\n",
    "            image = Image.open(filename)\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.imagesData = images        \n",
    "        self.labelsData = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagesData)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'image': self.imagesData[idx], 'label': self.labelsData[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            sample['label'] = torch.from_numpy(np.array(sample['label'], dtype='long')).long()           \n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceFeelLoadDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    # Read a PGM file and return a list of lists (each list is a line)\n",
    "    def read_pgm(self, pgmf):\n",
    "        assert pgmf.readline() == b'P5\\n'\n",
    "        (width, height) = [int(i) for i in pgmf.readline().split()]\n",
    "        depth = int(pgmf.readline())\n",
    "        assert depth <= 255\n",
    "\n",
    "        raster = []\n",
    "        for y in range(height):\n",
    "            for y in range(width):\n",
    "                raster.append(np.array(ord(pgmf.read(1))))\n",
    "        return np.array(raster)\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        # Read all images to build dataset\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for filename in glob.glob(root_dir + \"*.pgm\"):\n",
    "            if \"angry\" in filename:\n",
    "                label = 0\n",
    "            elif \"happy\" in filename:\n",
    "                label = 1\n",
    "            elif \"neutral\" in filename:\n",
    "                label = 2\n",
    "            elif \"sad\" in filename:\n",
    "                label = 3\n",
    "            image = Image.open(filename)\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.imagesData = images        \n",
    "        self.labelsData = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagesData)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'image': self.imagesData[idx], 'label': self.labelsData[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            sample['label'] = torch.from_numpy(np.array(sample['label'], dtype='long')).long()           \n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32,padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "\n",
    "dataset = FaceFeelLoadDataset(root_dir='faces_4/*/', transform=preprocess)\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "validation_split = .1\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "validation_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "validation_loader = DataLoader(dataset, batch_size=batch_size, sampler=validation_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=12)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=12)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)       \n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=24)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=24)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=24, out_channels=48, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=48)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(in_channels=48, out_channels=48, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(num_features=48)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(num_features=96)\n",
    "        self.relu7 = nn.ReLU()\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(in_channels=96, out_channels=96, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(num_features=96)\n",
    "        self.relu8 = nn.ReLU()        \n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d (kernel_size = 4)\n",
    "                       \n",
    "\n",
    "        self.fc = nn.Linear(in_features= 96, out_features=num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = self.relu2(output)\n",
    "\n",
    "        output = self.pool1(output)\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "\n",
    "        output = self.conv4(output)\n",
    "        output = self.bn4(output)\n",
    "        output = self.relu4(output)\n",
    "         \n",
    "        output = self.pool2(output)\n",
    "        \n",
    "        output = self.conv5(output)\n",
    "        output = self.bn5(output)\n",
    "        output = self.relu5(output)\n",
    "        \n",
    "        output = self.conv6(output)\n",
    "        output = self.bn6(output)\n",
    "        output = self.relu6(output)  \n",
    "        \n",
    "        output = self.pool3(output)\n",
    "        \n",
    "        output = self.conv7(output)\n",
    "        output = self.bn7(output)\n",
    "        output = self.relu7(output)\n",
    "        \n",
    "        output = self.conv8(output)\n",
    "        output = self.bn8(output)\n",
    "        output = self.relu8(output)  \n",
    "        \n",
    "        output = self.avgpool(output)\n",
    "\n",
    "        output = output.view(-1, 96)\n",
    "\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "net = Net(4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Letâ€™s use a Classification Cross-Entropy loss and SGD with momentum.\n",
    "from torch.optim import Adam\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = Adam(net.parameters(), lr=0.001, weight_decay=0.0001) #optim.SGD(net.parameters(), lr=0.00001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (conv4): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv5): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn5): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu5): ReLU()\n",
       "  (conv6): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu6): ReLU()\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv7): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu7): ReLU()\n",
       "  (conv8): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu8): ReLU()\n",
       "  (avgpool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "  (fc): Linear(in_features=96, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"modelo_feel\")\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "net.eval() # - or - model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.386\n",
      "[2,   100] loss: 1.388\n",
      "[3,   100] loss: 1.388\n",
      "[4,   100] loss: 1.386\n",
      "[5,   100] loss: 1.386\n",
      "[6,   100] loss: 1.386\n",
      "[7,   100] loss: 1.385\n",
      "[8,   100] loss: 1.387\n",
      "[9,   100] loss: 1.387\n",
      "[10,   100] loss: 1.388\n",
      "[11,   100] loss: 1.387\n",
      "[12,   100] loss: 1.385\n",
      "[13,   100] loss: 1.387\n",
      "[14,   100] loss: 1.386\n",
      "[15,   100] loss: 1.385\n",
      "[16,   100] loss: 1.386\n",
      "[17,   100] loss: 1.387\n",
      "[18,   100] loss: 1.386\n",
      "[19,   100] loss: 1.387\n",
      "[20,   100] loss: 1.387\n",
      "[21,   100] loss: 1.386\n",
      "[22,   100] loss: 1.387\n",
      "[23,   100] loss: 1.385\n",
      "[24,   100] loss: 1.386\n",
      "[25,   100] loss: 1.386\n",
      "[26,   100] loss: 1.386\n",
      "[27,   100] loss: 1.386\n",
      "[28,   100] loss: 1.385\n",
      "[29,   100] loss: 1.385\n",
      "[30,   100] loss: 1.385\n",
      "[31,   100] loss: 1.387\n",
      "[32,   100] loss: 1.387\n",
      "[33,   100] loss: 1.387\n",
      "[34,   100] loss: 1.386\n",
      "[35,   100] loss: 1.387\n",
      "[36,   100] loss: 1.385\n",
      "[37,   100] loss: 1.387\n",
      "[38,   100] loss: 1.386\n",
      "[39,   100] loss: 1.387\n",
      "[40,   100] loss: 1.387\n",
      "[41,   100] loss: 1.387\n",
      "[42,   100] loss: 1.387\n",
      "[43,   100] loss: 1.387\n",
      "[44,   100] loss: 1.387\n",
      "[45,   100] loss: 1.386\n",
      "[46,   100] loss: 1.387\n",
      "[47,   100] loss: 1.387\n",
      "[48,   100] loss: 1.386\n",
      "[49,   100] loss: 1.385\n",
      "[50,   100] loss: 1.385\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "       \n",
    "        # get the inputs\n",
    "        inputs, labels = data[\"image\"], data[\"label\"]\n",
    "       \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss}, \"modelo_feel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 25 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        images, labels = data[\"image\"], data[\"label\"]\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
