{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceGlassLoadDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    # Read a PGM file and return a list of lists (each list is a line)\n",
    "    def read_pgm(self, pgmf):\n",
    "        assert pgmf.readline() == b'P5\\n'\n",
    "        (width, height) = [int(i) for i in pgmf.readline().split()]\n",
    "        depth = int(pgmf.readline())\n",
    "        assert depth <= 255\n",
    "\n",
    "        raster = []\n",
    "        for y in range(height):\n",
    "            for y in range(width):\n",
    "                raster.append(np.array(ord(pgmf.read(1))))\n",
    "        return np.array(raster)\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        # Read all images to build dataset\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for filename in glob.glob(root_dir + \"*.pgm\"):\n",
    "            if \"sunglasses\" in filename:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            image = Image.open(filename)\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.imagesData = images        \n",
    "        self.labelsData = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagesData)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'image': self.imagesData[idx], 'label': self.labelsData[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            sample['label'] = torch.from_numpy(np.array(sample['label'], dtype='long')).long()           \n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDirectionLoadDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    # Read a PGM file and return a list of lists (each list is a line)\n",
    "    def read_pgm(self, pgmf):\n",
    "        assert pgmf.readline() == b'P5\\n'\n",
    "        (width, height) = [int(i) for i in pgmf.readline().split()]\n",
    "        depth = int(pgmf.readline())\n",
    "        assert depth <= 255\n",
    "\n",
    "        raster = []\n",
    "        for y in range(height):\n",
    "            for y in range(width):\n",
    "                raster.append(np.array(ord(pgmf.read(1))))\n",
    "        return np.array(raster)\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        # Read all images to build dataset\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for filename in glob.glob(root_dir + \"*.pgm\"):\n",
    "            if \"left\" in filename:\n",
    "                label = 0\n",
    "            elif \"right\" in filename:\n",
    "                label = 1\n",
    "            elif \"straight\" in filename:\n",
    "                label = 2\n",
    "            elif \"up\" in filename:\n",
    "                label = 3\n",
    "            image = Image.open(filename)\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.imagesData = images        \n",
    "        self.labelsData = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagesData)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'image': self.imagesData[idx], 'label': self.labelsData[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            sample['label'] = torch.from_numpy(np.array(sample['label'], dtype='long')).long()           \n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceFeelLoadDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    # Read a PGM file and return a list of lists (each list is a line)\n",
    "    def read_pgm(self, pgmf):\n",
    "        assert pgmf.readline() == b'P5\\n'\n",
    "        (width, height) = [int(i) for i in pgmf.readline().split()]\n",
    "        depth = int(pgmf.readline())\n",
    "        assert depth <= 255\n",
    "\n",
    "        raster = []\n",
    "        for y in range(height):\n",
    "            for y in range(width):\n",
    "                raster.append(np.array(ord(pgmf.read(1))))\n",
    "        return np.array(raster)\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        # Read all images to build dataset\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for filename in glob.glob(root_dir + \"*.pgm\"):\n",
    "            if \"angry\" in filename:\n",
    "                label = 0\n",
    "            elif \"happy\" in filename:\n",
    "                label = 1\n",
    "            elif \"neutral\" in filename:\n",
    "                label = 2\n",
    "            elif \"sad\" in filename:\n",
    "                label = 3\n",
    "            image = Image.open(filename)\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.imagesData = images        \n",
    "        self.labelsData = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagesData)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'image': self.imagesData[idx], 'label': self.labelsData[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            sample['label'] = torch.from_numpy(np.array(sample['label'], dtype='long')).long()           \n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32,padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "\n",
    "dataset = FaceFeelLoadDataset(root_dir='faces_4/*/', transform=preprocess)\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "validation_split = .1\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "validation_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "validation_loader = DataLoader(dataset, batch_size=batch_size, sampler=validation_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=12)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=12)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)       \n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=24)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=24)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=24, out_channels=48, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=48)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(in_channels=48, out_channels=48, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(num_features=48)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(num_features=96)\n",
    "        self.relu7 = nn.ReLU()\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(in_channels=96, out_channels=96, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(num_features=96)\n",
    "        self.relu8 = nn.ReLU()        \n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d (kernel_size = 4)\n",
    "                       \n",
    "\n",
    "        self.fc = nn.Linear(in_features= 96, out_features=num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = self.relu2(output)\n",
    "\n",
    "        output = self.pool1(output)\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "\n",
    "        output = self.conv4(output)\n",
    "        output = self.bn4(output)\n",
    "        output = self.relu4(output)\n",
    "         \n",
    "        output = self.pool2(output)\n",
    "        \n",
    "        output = self.conv5(output)\n",
    "        output = self.bn5(output)\n",
    "        output = self.relu5(output)\n",
    "        \n",
    "        output = self.conv6(output)\n",
    "        output = self.bn6(output)\n",
    "        output = self.relu6(output)  \n",
    "        \n",
    "        output = self.pool3(output)\n",
    "        \n",
    "        output = self.conv7(output)\n",
    "        output = self.bn7(output)\n",
    "        output = self.relu7(output)\n",
    "        \n",
    "        output = self.conv8(output)\n",
    "        output = self.bn8(output)\n",
    "        output = self.relu8(output)  \n",
    "        \n",
    "        output = self.avgpool(output)\n",
    "\n",
    "        output = output.view(-1, 96)\n",
    "\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "net = Net(4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Letâ€™s use a Classification Cross-Entropy loss and SGD with momentum.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.00001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.391\n",
      "[2,   100] loss: 1.388\n",
      "[3,   100] loss: 1.387\n",
      "[4,   100] loss: 1.391\n",
      "[5,   100] loss: 1.394\n",
      "[6,   100] loss: 1.389\n",
      "[7,   100] loss: 1.388\n",
      "[8,   100] loss: 1.387\n",
      "[9,   100] loss: 1.388\n",
      "[10,   100] loss: 1.390\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "       \n",
    "        # get the inputs\n",
    "        inputs, labels = data[\"image\"], data[\"label\"]\n",
    "       \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 22 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        images, labels = data[\"image\"], data[\"label\"]\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
